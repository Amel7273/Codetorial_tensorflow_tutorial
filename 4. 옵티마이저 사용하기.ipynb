{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"collapsed_sections":[],"mount_file_id":"1pxDEu8a3NZHes6HX7wKQ4ksb-P7_U7Nr","authorship_tag":"ABX9TyOC4p+fLTpkl7dhURyuTo2r"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#4. 옵티마이저 사용하기"],"metadata":{"id":"UgqegGAf6Lpj"}},{"cell_type":"markdown","source":["<img src = \"https://codetorial.net/tensorflow/_images/basics_of_optimizer_00.png\" width = 500 height = 400/>"],"metadata":{"id":"WsG587CbWAlK"}},{"cell_type":"markdown","source":["옵티마이저 (Optimizer)는 손실 함수를 통해 얻은 손실값으로부터 모델을 업데이트하는 방식을 의미합니다.\n","\n","TensorFlow는 SGD, Adam, RMSprop와 같은 다양한 종류의 옵티마이저를 제공합니다.\n","\n","옵티마이저의 기본 사용법을 알아보고, 훈련 과정에서 옵티마이저에 따라 모델의 손실갑싱 어떻게 감소하는지 확인해 보겠습니다."],"metadata":{"id":"7Y3FRklOWAmo"}},{"cell_type":"markdown","source":["### Table of Contents\n","\n","1. Neural Network 구성하기\n","2. Neural Network 컴파일하기\n","3. Neural Network 훈련하기\n","4. 손실값 시각화하기\n","5. 출력값 시각화하기\n","6. 옵티마이저 비교하기"],"metadata":{"id":"brxLhk_nXBIU"}},{"cell_type":"markdown","source":["## 1) Neural Network 구성하기\n","\n","이전 페이지에서와 마찬가지로 1개의 입력, 3개의 출력 노드를 갖는 신경망 모델을 구성합니다.\n","\n","<img src = \"https://codetorial.net/tensorflow/_images/basics_of_optimizer_01.png\" />"],"metadata":{"id":"6PMcaVlGXn5O"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras\n","import numpy as np\n","\n","tf.random.set_seed(0)\n","\n","model = keras.Sequential([keras.layers.Dense(units=3, input_shape=[1])])"],"metadata":{"id":"Tozeu2MyW_Xi"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이번에는 tf.random 모듈의 set_seed() 함수를 사용해서 랜덤 시드를 설정했습니다.\n","\n","tf.keras 모듈의 Sequential 클래스는 Neural Network의 각 층을 순서대로 쌓을 수 있도록 합니다."],"metadata":{"id":"fdtyzaf2Yar3"}},{"cell_type":"markdown","source":["## 2) Neural Network 컴파일하기 \n","\n","구성한 모델의 손실 함수와 옵티마이저를 지정하기 위해 compile() 메서드를 사용합니다."],"metadata":{"id":"pj-XVGC5ZcXq"}},{"cell_type":"code","source":["model.compile(loss='mse', optimizer='SGD')"],"metadata":{"id":"ljzeArhYYTw2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["손실 함수로 'mse'를, 옵티마이저로 'SGD'을 지정했습니다.\n","\n","'SGD'는 Stochastic Gradient Descent의 줄임말이며, 우리말로는 확률적 경사하강법이라고 부릅니다."],"metadata":{"id":"4ob0cIzsZqga"}},{"cell_type":"markdown","source":["## 3) Neural Network 훈련하기\n","\n","fit() 메서드는 컴파일 과정에서 지정한 손실 함수와 옵티마이저를 사용해서 모델을 훈련합니다."],"metadata":{"id":"5qDLV_3xZ5M0"}},{"cell_type":"code","source":["model.fit([1], [[0, 1, 0]], epochs=1)\n","model.evaluate([1], [[0, 1, 0]])"],"metadata":{"id":"2XRBze5yZp2g"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["fit() 메서드는 훈련 진행 상황과 현재의 손실값을 반환합니다.\n","\n","1회의 에포크(epoch) 이후, evaluate() 메서드를 사용해서 손실값을 확인해보면\n","\n","손실값이 1.07에서 1.04로 감소했음을 알 수 있습니다."],"metadata":{"id":"dwZpKIn4aQ7n"}},{"cell_type":"code","source":["history = model.fit([1],[[0,1,0]], epochs = 100)"],"metadata":{"id":"RXiOL4i1aMvI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이번에는 훈련의 에포크를 100회로 지정했습니다.\n","\n","100회의 훈련 과정의 훈련 시간과 손실값이 출력됩니다."],"metadata":{"id":"jaJwIwJ0asrd"}},{"cell_type":"markdown","source":["## 4) 손실값 시각화하기"],"metadata":{"id":"Fcp19i9yay1f"}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","\n","plt.style.use('default')\n","plt.rcParams['figure.figsize'] = (4,3)\n","plt.rcParams['font.size'] = 12\n","\n","loss = history.history['loss']\n","plt.plot(loss)\n","plt.xlabel('Epoch') \n","plt.ylabel('Loss')\n","plt.show()"],"metadata":{"id":"8U2HftIEaocj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["fit() 메서드는 history 객체를 반환합니다.\n","\n","history 객체의 history 속성은 손실값(loss values)과 지표(metrics)를 포함합니다.\n","\n","컴파일 과정에서 지표를 지정하지 않았기 때문에 이 예제의 history 속성은 지표(metrics)를 포함하지 않습니다.\n","\n","훈련 과정의 손실값을 Matplotlib을 이용해서 그래프로 나타내면 아래와 같이 감소하는 경함을 확인할 수 있습니다."],"metadata":{"id":"VIuO8tojbL7S"}},{"cell_type":"markdown","source":["## 5) 출력값 시각화하기"],"metadata":{"id":"p2av1nksb3VR"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow import keras \n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","plt.style.use('default')\n","plt.rcParams['figure.figsize'] = (4,3)\n","plt.rcParams['font.size'] = 12\n","\n","tf.random.set_seed(0)\n","\n","model = keras.Sequential([keras.layers.Dense(units=3, input_shape=[1], use_bias=False)])\n","model.compile(loss='mse', optimizer='SGD')\n","\n","pred = model.predict([1])\n","print(pred)\n","print(model.get_weights())\n","\n","plt.bar(np.arange(3), pred[0])\n","plt.ylim(-1.1, 1.1)\n","plt.xlabel('Output Node')\n","plt.ylabel('Output')\n","plt.text(-0.4,0.8,'Epoch 0')\n","plt.tight_layout()\n","plt.savefig('/content/drive/MyDrive/Colab Notebooks/img/plt/pred000.png')\n","plt.clf()\n","\n","epochs = 500\n","for i in range(1, epochs+1):\n","  model.fit([1], [[0,1,0]], epochs=1, verbose=0)\n","  pred = model.predict([1])\n","\n","  if i % 25 == 0:\n","    plt.bar(np.arange(3), pred[0])\n","    plt.ylim(-1.1, 1.1)\n","    plt.xlabel('Output Node')\n","    plt.ylabel('Output')\n","    plt.text(-0.4,0.8,'Epoch ' + str(i))\n","    plt.tight_layout()\n","    plt.savefig('/content/drive/MyDrive/Colab Notebooks/img/plt/pred' + str(i).zfill(3) + '.png')\n","    plt.clf()\n","\n","print(pred)\n","print(model.get_weights())"],"metadata":{"id":"VLYEpWxObIKv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["이 코드는 Matplotlib을 이용해서 500회의 에포크 동안 훈련에 의해 출력값이 변화하는 과정을 시각화합니다.\n","\n","Matplotlib의 다양한 함수에 대해서는 Matplotlib - 파이썬으로 그래프 그리기를 참고하세요.\n","\n","아래 그림과 같이 훈련 과정 동안 출력값이 Target 값 [0,1,0]에 가까워지는 것을 알 수 있습니다."],"metadata":{"id":"UrensG-VANfo"}},{"cell_type":"markdown","source":["## 6) 옵티마이저 비교하기\n","\n","아래의 예제는 세 가지 옵티마이저 'SGD', 'Adam', 'RMSprop'이 모델을 업데이트하는 성능을 비교합니다."],"metadata":{"id":"uer-siYBAkB0"}},{"cell_type":"code","source":["import tensorflow as tf \n","from tensorflow import keras \n","import numpy as np \n","import matplotlib.pyplot as plt\n","\n","plt.style.use('default')\n","plt.rcParams['figure.figsize'] = (4,3)\n","plt.rcParams['font.size'] = 12\n","\n","tf.random.set_seed(0)\n","model = keras.Sequential([keras.layers.Dense(units=3,input_shape=[1])])\n","\n","tf.random.set_seed(0)\n","model2 = tf.keras.models.clone_model(model)\n","\n","tf.random.set_seed(0)\n","model3 = tf.keras.models.clone_model(model)\n","\n","model.compile(loss='mse', optimizer='SGD')\n","model2.compile(loss='mse', optimizer='Adam')\n","model3.compile(loss='mse', optimizer='RMSprop')\n","\n","history = model.fit([1],[[0, 1, 0]], epochs=100, verbose=0)\n","history2 = model2.fit([1],[[0, 1, 0]], epochs=100, verbose=0)\n","history3 = model3.fit([1],[[0, 1, 0]], epochs=100, verbose=0)\n","\n","loss = history.history['loss']\n","loss2 = history2.history['loss']\n","loss3 = history3.history['loss']\n","\n","plt.plot(loss, label='SGD')\n","plt.plot(loss2, label='Adam')\n","plt.plot(loss3, label='RMSprop')\n","\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.legend(loc='lower left')\n","plt.show()"],"metadata":{"id":"RPLEpa6RcB9Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["우선 세 개의 모델이 동일한 가중치 값을 갖도록 하기 위해 set_seed() 함수를 세 번 호출했습니다.\n","\n","compile()메서드에서 각각 다른 옵티마이저를 지정합니다.\n","\n","100회의 훈련 과정의 손실값을 시각화하면 아래와 같습니다.\n","\n","옵티마이저에 따라 모델을 업데이트하는 방식과 손실값이 감소하는 경향에 차이가 있음을 알 수 있습니다."],"metadata":{"id":"mMw_eobTCDVj"}},{"cell_type":"code","source":[],"metadata":{"id":"FRM72syyB0-z"},"execution_count":null,"outputs":[]}]}